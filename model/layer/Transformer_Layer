import numpy as np
import torch
import torch.nn as nn
import math


def get_attn_subsequence_mask(seq):
    '''
    seq: [batch_size, tgt_len]
    '''
    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]   # (20,
    subsequence_mask = np.triu(np.ones(attn_shape), k=1) # Upper triangular matrix
    subsequence_mask = torch.from_numpy(subsequence_mask).byte()
    return subsequence_mask

# class Transformer_Decoder_Layer(nn.Module):
#     def __init__(self, channel, reduction=64):
#         super(Transformer_Decoder_Layer, self).__init__()

class PositionalEncoding(nn.Module):
    def __init__(self, time_lag, station_num, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.d_model = time_lag
        pe = torch.zeros(station_num, time_lag)  # (5, 276)

        for pos in range(station_num):
            for i in range(0, time_lag-1, 2):
                # print(i)
                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i) / time_lag)))
                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / time_lag)))
        pe = pe.unsqueeze(0)  # (1, 64, 30)
        self.register_buffer("pe", pe)

    def forward(self, x):
        # print(x.shape)
        # x = x.permute(0, 2, 1)
        x = x * math.sqrt(self.d_model)   # ([20, 64, 30])
        x = x + self.pe[:x.size(0), :]
        # x = x.permute(0, 2, 1)
        return self.dropout(x)

class LayerNorm(nn.Module):    # Normalization
    "Construct a layernorm module (See citation for details)."
    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2


class SublayerConnection(nn.Module):  # Connection
    """
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    """
    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        "Apply residual connection to any sublayer with the same size."
        return x + self.dropout(sublayer(self.norm(x)))
